{
    "Vision Transformer processes images by dividing them into ____ patches": "16x16",
    "In ViT, the ____ token embedding is used for image classification": "[class] or classification",
    "CLIP stands for ____": "Contrastive Language-Image Pre-Training",
    "CLIP aligns text and image embeddings using ____ loss": "contrastive or cross-entropy",
    "The dataset used to train CLIP contains approximately ____ image-caption pairs": "400 million",
    "LLaVA uses ____ as its image encoder": "CLIP",
    "LLaVA uses ____ as its language model": "Vicuna",
    "The layer that connects CLIP image embeddings to the language model in LLaVA is called ____": "linear projection",
    "Why does ViT add position embeddings to patch embeddings?": "To preserve spatial information about patch locations",
    "Why does CLIP use contrastive learning instead of predicting exact captions?": "More efficient and scalable than generating full captions",
    "Why is zero-shot CLIP more robust to natural distribution shift than standard ImageNet models?": "Trained on diverse web data rather than curated datasets",
    "Why does LLaVA perform pre-training before instruction tuning?": "To align image embeddings with word embeddings",
    "The pre-training objective for ViT is ____": "image classification",
    "Zero-shot CLIP performs comparably to supervised models with ____ labeled examples per class": "16",
    "LLaVA generates instruction-tuning data using ____": "GPT-4",
    "The three types of instruction data used to train LLaVA are: conversations, detailed descriptions, and ____": "complex reasoning",
    "During LLaVA training, the ____ encoder remains frozen": "CLIP image",
    "CLIP cannot properly handle ____ in text queries": "word order or syntax",
    "Why does LLaVA use GPT-4 to generate training data rather than human annotations?": "Scalable and cost-effective way to create diverse instruction-response pairs",
    "What information might CLIP embeddings fail to capture that limits LLaVA's capabilities?": "Fine-grained spatial relationships or detailed visual context",
    "The joint embedding space allows models to ____": "process both images and text using same representations",
    "ViT pre-training learns image representations useful for ____": "distinguishing between different objects"
  }